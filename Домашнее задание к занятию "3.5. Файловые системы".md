# Домашнее задание к занятию "3.5. Файловые системы"

1. Узнайте о [sparse](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B7%D1%80%D0%B5%D0%B6%D1%91%D0%BD%D0%BD%D1%8B%D0%B9_%D1%84%D0%B0%D0%B9%D0%BB) (разряженных) файлах.

Готово

1. Могут ли файлы, являющиеся жесткой ссылкой на один объект, иметь разные права доступа и владельца? Почему?  
Нет, жесткая ссылка имеет тот же inode что и файл, то и права будут идентичными

1. Сделайте `vagrant destroy` на имеющийся инстанс Ubuntu. Замените содержимое Vagrantfile следующим:

    ```bash
    Vagrant.configure("2") do |config|
      config.vm.box = "bento/ubuntu-20.04"
      config.vm.provider :virtualbox do |vb|
        lvm_experiments_disk0_path = "/tmp/lvm_experiments_disk0.vmdk"
        lvm_experiments_disk1_path = "/tmp/lvm_experiments_disk1.vmdk"
        vb.customize ['createmedium', '--filename', lvm_experiments_disk0_path, '--size', 2560]
        vb.customize ['createmedium', '--filename', lvm_experiments_disk1_path, '--size', 2560]
        vb.customize ['storageattach', :id, '--storagectl', 'SATA Controller', '--port', 1, '--device', 0, '--type', 'hdd', '--medium', lvm_experiments_disk0_path]
        vb.customize ['storageattach', :id, '--storagectl', 'SATA Controller', '--port', 2, '--device', 0, '--type', 'hdd', '--medium', lvm_experiments_disk1_path]
      end
    end
    ```

    Данная конфигурация создаст новую виртуальную машину с двумя дополнительными неразмеченными дисками по 2.5 Гб.  
    
   Готово

1. Используя `fdisk`, разбейте первый диск на 2 раздела: 2 Гб, оставшееся пространство.  
vagrant@vagrant:~$ lsblk  
NAME                 MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT  
sda                    8:0    0   64G  0 disk  
├─sda1                 8:1    0  512M  0 part /boot/efi  
├─sda2                 8:2    0    1K  0 part  
└─sda5                 8:5    0 63.5G  0 part  
  ├─vgvagrant-root   253:0    0 62.6G  0 lvm  /  
  └─vgvagrant-swap_1 253:1    0  980M  0 lvm  [SWAP]  
sdb                    8:16   0  2.5G  0 disk  
sdc                    8:32   0  2.5G  0 disk  

С помощью fdisk таблица разделов приведена к слудующему виду:  

root@vagrant:~# lsblk  
NAME                 MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT  
sda                    8:0    0   64G  0 disk  
├─sda1                 8:1    0  512M  0 part /boot/efi  
├─sda2                 8:2    0    1K  0 part  
└─sda5                 8:5    0 63.5G  0 part  
  ├─vgvagrant-root   253:0    0 62.6G  0 lvm  /  
  └─vgvagrant-swap_1 253:1    0  980M  0 lvm  [SWAP]  
sdb                    8:16   0  2.5G  0 disk  
├─sdb1                 8:17   0    2G  0 part  
└─sdb2                 8:18   0  511M  0 part  
sdc                    8:32   0  2.5G  0 disk  


1. Используя `sfdisk`, перенесите данную таблицу разделов на второй диск.  

sfdisk -d /dev/sdb | sfdisk /dev/sdc    

root@vagrant:~# lsblk    
NAME                 MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT  
sda                    8:0    0   64G  0 disk  
├─sda1                 8:1    0  512M  0 part /boot/efi  
├─sda2                 8:2    0    1K  0 part  
└─sda5                 8:5    0 63.5G  0 part  
  ├─vgvagrant-root   253:0    0 62.6G  0 lvm  /  
  └─vgvagrant-swap_1 253:1    0  980M  0 lvm  [SWAP]  
sdb                    8:16   0  2.5G  0 disk  
├─sdb1                 8:17   0    2G  0 part  
└─sdb2                 8:18   0  511M  0 part  
sdc                    8:32   0  2.5G  0 disk  
├─sdc1                 8:33   0    2G  0 part  
└─sdc2                 8:34   0  511M  0 part  

1. Соберите `mdadm` RAID1 на паре разделов 2 Гб.

root@vagrant:~# mdadm --create --verbose /dev/md1 -l 1 -n 2 /dev/sdb1 /dev/sdc1  (/dev/md1 выбрал для удобства (raid1)  
Результат:  

root@vagrant:~# lsblk  
NAME                 MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT  
sda                    8:0    0   64G  0 disk  
├─sda1                 8:1    0  512M  0 part  /boot/efi  
├─sda2                 8:2    0    1K  0 part  
└─sda5                 8:5    0 63.5G  0 part  
  ├─vgvagrant-root   253:0    0 62.6G  0 lvm   /  
  └─vgvagrant-swap_1 253:1    0  980M  0 lvm   [SWAP]  
sdb                    8:16   0  2.5G  0 disk  
├─sdb1                 8:17   0    2G  0 part  
│ └─md1                9:1    0    2G  0 raid1  
└─sdb2                 8:18   0  511M  0 part  
sdc                    8:32   0  2.5G  0 disk  
├─sdc1                 8:33   0    2G  0 part  
│ └─md1                9:1    0    2G  0 raid1  
└─sdc2                 8:34   0  511M  0 part  


1. Соберите `mdadm` RAID0 на второй паре маленьких разделов.  

 mdadm --create --verbose /dev/md0 -l 0 -n 2 /dev/sdb2 /dev/sdc2  
 Результат:  
 
root@vagrant:~# lsblk  
NAME                 MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT  
sda                    8:0    0   64G  0 disk  
├─sda1                 8:1    0  512M  0 part  /boot/efi  
├─sda2                 8:2    0    1K  0 part  
└─sda5                 8:5    0 63.5G  0 part  
  ├─vgvagrant-root   253:0    0 62.6G  0 lvm   /  
  └─vgvagrant-swap_1 253:1    0  980M  0 lvm   [SWAP]  
sdb                    8:16   0  2.5G  0 disk  
├─sdb1                 8:17   0    2G  0 part  
│ └─md1                9:1    0    2G  0 raid1  
└─sdb2                 8:18   0  511M  0 part  
  └─md0                9:0    0 1018M  0 raid0  
sdc                    8:32   0  2.5G  0 disk  
├─sdc1                 8:33   0    2G  0 part  
│ └─md1                9:1    0    2G  0 raid1  
└─sdc2                 8:34   0  511M  0 part  
  └─md0                9:0    0 1018M  0 raid0  


1. Создайте 2 независимых PV на получившихся md-устройствах.  

 pvcreate /dev/md0 /dev/md1

1. Создайте общую volume-group на этих двух PV.  

vgcreate vg0 /dev/md0 /dev/md1

1. Создайте LV размером 100 Мб, указав его расположение на PV с RAID0.  

lvcreate -L 100M /dev/vg0 /dev/md0  

 --- Logical volume ---  
  LV Path                /dev/vg0/lvol0  
  LV Name                lvol0  
  VG Name                vg0  
  LV UUID                jgXII8-APp3-IulO-4wNU-i4uJ-iFz1-btJaP0  
  LV Write Access        read/write  
  LV Creation host, time vagrant, 2021-12-10 15:47:19 +0000  
  LV Status              available  
  # open                 0  
  LV Size                100.00 MiB  
  Current LE             25  
  Segments               1  
  Allocation             inherit  
  Read ahead sectors     auto  
  - currently set to     4096  
  Block device           253:2  


1. Создайте `mkfs.ext4` ФС на получившемся LV.

mkfs.ext4 /dev/vg0/lvol0  

1. Смонтируйте этот раздел в любую директорию, например, `/tmp/new`.  

root@vagrant:~# mkdir /tmp/new  
root@vagrant:~# mount /dev/vg0/lvol0 /tmp/new  


1. Поместите туда тестовый файл, например `wget https://mirror.yandex.ru/ubuntu/ls-lR.gz -O /tmp/new/test.gz`.  

root@vagrant:~# ls /tmp/new/  
lost+found  test.gz  


1. Прикрепите вывод `lsblk`.  

root@vagrant:~# lsblk  
NAME                 MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT  
sda                    8:0    0   64G  0 disk  
├─sda1                 8:1    0  512M  0 part  /boot/efi  
├─sda2                 8:2    0    1K  0 part  
└─sda5                 8:5    0 63.5G  0 part  
  ├─vgvagrant-root   253:0    0 62.6G  0 lvm   /  
  └─vgvagrant-swap_1 253:1    0  980M  0 lvm   [SWAP]  
sdb                    8:16   0  2.5G  0 disk  
├─sdb1                 8:17   0    2G  0 part  
│ └─md1                9:1    0    2G  0 raid1  
└─sdb2                 8:18   0  511M  0 part  
  └─md0                9:0    0 1018M  0 raid0  
    └─vg0-lvol0      253:2    0  100M  0 lvm   /tmp/new  
sdc                    8:32   0  2.5G  0 disk  
├─sdc1                 8:33   0    2G  0 part  
│ └─md1                9:1    0    2G  0 raid1  
└─sdc2                 8:34   0  511M  0 part  
  └─md0                9:0    0 1018M  0 raid0  
    └─vg0-lvol0      253:2    0  100M  0 lvm   /tmp/new  


1. Протестируйте целостность файла:

    ```bash
    root@vagrant:~# gzip -t /tmp/new/test.gz
    root@vagrant:~# echo $?
    0
    ```  
    
    
root@vagrant:~# gzip -t /tmp/new/test.gz  
root@vagrant:~# echo $?  
0  


1. Используя pvmove, переместите содержимое PV с RAID0 на RAID1.  

root@vagrant:~# man pvmove  
root@vagrant:~# pvmove /dev/md0  
  /dev/md0: Moved: 24.00%  
  /dev/md0: Moved: 100.00%  
root@vagrant:~# lsblk  
NAME                 MAJ:MIN RM  SIZE RO TYPE  MOUNTPOINT  
sda                    8:0    0   64G  0 disk  
├─sda1                 8:1    0  512M  0 part  /boot/efi  
├─sda2                 8:2    0    1K  0 part  
└─sda5                 8:5    0 63.5G  0 part  
  ├─vgvagrant-root   253:0    0 62.6G  0 lvm   /  
  └─vgvagrant-swap_1 253:1    0  980M  0 lvm   [SWAP]  
sdb                    8:16   0  2.5G  0 disk  
├─sdb1                 8:17   0    2G  0 part  
│ └─md1                9:1    0    2G  0 raid1  
│   └─vg0-lvol0      253:2    0  100M  0 lvm   /tmp/new  
└─sdb2                 8:18   0  511M  0 part  
  └─md0                9:0    0 1018M  0 raid0  
sdc                    8:32   0  2.5G  0 disk  
├─sdc1                 8:33   0    2G  0 part  
│ └─md1                9:1    0    2G  0 raid1  
│   └─vg0-lvol0      253:2    0  100M  0 lvm   /tmp/new  
└─sdc2                 8:34   0  511M  0 part  
  └─md0                9:0    0 1018M  0 raid0  



1. Сделайте `--fail` на устройство в вашем RAID1 md.

root@vagrant:~# mdadm -f /dev/md1 /dev/sdb1  
mdadm: set /dev/sdb1 faulty in /dev/md1  


1. Подтвердите выводом `dmesg`, что RAID1 работает в деградированном состоянии.

[ 9312.404264] md/raid1:md1: Disk failure on sdb1, disabling device.  
               md/raid1:md1: Operation continuing on 1 devices.  


1. Протестируйте целостность файла, несмотря на "сбойный" диск он должен продолжать быть доступен:

    ```bash
    root@vagrant:~# gzip -t /tmp/new/test.gz
    root@vagrant:~# echo $?
    0
    ```

root@vagrant:~# gzip -t /tmp/new/test.gz  
root@vagrant:~# echo $?  
0  



1. Погасите тестовый хост, `vagrant destroy`.  
Готово
